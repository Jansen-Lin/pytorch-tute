{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# ###########################################################################################################\n",
      "#  NLP with PyTorch - Encoding Text Data\n",
      "# ###########################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "# ###########################################################################################################\n",
    "#  NLP with PyTorch - Encoding Text Data\n",
    "# ###########################################################################################################\n",
    "''')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../../../notebooks/Data/shakespeare.txt', mode='r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "5445609\n"
     ]
    }
   ],
   "source": [
    "print(type(text))\n",
    "print(len(text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bud buriest thy content,\n",
      "  And tender churl mak'st waste in niggarding:\n",
      "    Pity the world, or else this glutton be,\n",
      "    To eat the world's due, by the grave and thee.\n",
      "\n",
      "\n",
      "                     2\n",
      "  When forty winters shall besiege thy brow,\n",
      "  And dig deep trenches in thy beauty's field,\n",
      "  Thy youth's proud livery so gazed on now,\n",
      "  Will be a tattered weed of small worth held:  \n",
      "  Then being asked, where all thy beauty lies,\n",
      "  Where all the treasure of thy lusty days;\n",
      "  To say within thine own deep su\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "{'F', 'z', 'h', 'u', 'm', 'U', 'k', ':', 'X', 'g', 'W', 'I', 'Y', '!', ']', 'f', 'Z', 'a', '(', 'A', '8', 'P', '\"', 'R', ' ', \"'\", 'G', '6', 'l', 'K', '<', '>', 'p', 'o', ',', 'j', '\\n', 'b', '&', '.', 'H', '[', 'L', '7', '2', 'e', 'q', 'n', '|', 'i', 't', 'D', '`', '_', 'x', 'C', '-', 'y', '4', 'V', '3', 'c', 'J', 'r', 'S', 'E', 'd', 'M', 'N', 'T', 'v', 'w', ';', 'Q', '5', '1', ')', '0', 's', '}', 'O', '9', 'B', '?'}\n"
     ]
    }
   ],
   "source": [
    "# Derive unique characters from the text.\n",
    "all_unique_chars = set(text)\n",
    "print(len(all_unique_chars))\n",
    "print(all_unique_chars)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 'F',\n 1: 'z',\n 2: 'h',\n 3: 'u',\n 4: 'm',\n 5: 'U',\n 6: 'k',\n 7: ':',\n 8: 'X',\n 9: 'g',\n 10: 'W',\n 11: 'I',\n 12: 'Y',\n 13: '!',\n 14: ']',\n 15: 'f',\n 16: 'Z',\n 17: 'a',\n 18: '(',\n 19: 'A',\n 20: '8',\n 21: 'P',\n 22: '\"',\n 23: 'R',\n 24: ' ',\n 25: \"'\",\n 26: 'G',\n 27: '6',\n 28: 'l',\n 29: 'K',\n 30: '<',\n 31: '>',\n 32: 'p',\n 33: 'o',\n 34: ',',\n 35: 'j',\n 36: '\\n',\n 37: 'b',\n 38: '&',\n 39: '.',\n 40: 'H',\n 41: '[',\n 42: 'L',\n 43: '7',\n 44: '2',\n 45: 'e',\n 46: 'q',\n 47: 'n',\n 48: '|',\n 49: 'i',\n 50: 't',\n 51: 'D',\n 52: '`',\n 53: '_',\n 54: 'x',\n 55: 'C',\n 56: '-',\n 57: 'y',\n 58: '4',\n 59: 'V',\n 60: '3',\n 61: 'c',\n 62: 'J',\n 63: 'r',\n 64: 'S',\n 65: 'E',\n 66: 'd',\n 67: 'M',\n 68: 'N',\n 69: 'T',\n 70: 'v',\n 71: 'w',\n 72: ';',\n 73: 'Q',\n 74: '5',\n 75: '1',\n 76: ')',\n 77: '0',\n 78: 's',\n 79: '}',\n 80: 'O',\n 81: '9',\n 82: 'B',\n 83: '?'}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number -> letter (Decoder takes the number values and return respective character - basically a\n",
    "# lookup dictionary)\n",
    "decoder = dict(enumerate(all_unique_chars))\n",
    "decoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "{'F': 0,\n 'z': 1,\n 'h': 2,\n 'u': 3,\n 'm': 4,\n 'U': 5,\n 'k': 6,\n ':': 7,\n 'X': 8,\n 'g': 9,\n 'W': 10,\n 'I': 11,\n 'Y': 12,\n '!': 13,\n ']': 14,\n 'f': 15,\n 'Z': 16,\n 'a': 17,\n '(': 18,\n 'A': 19,\n '8': 20,\n 'P': 21,\n '\"': 22,\n 'R': 23,\n ' ': 24,\n \"'\": 25,\n 'G': 26,\n '6': 27,\n 'l': 28,\n 'K': 29,\n '<': 30,\n '>': 31,\n 'p': 32,\n 'o': 33,\n ',': 34,\n 'j': 35,\n '\\n': 36,\n 'b': 37,\n '&': 38,\n '.': 39,\n 'H': 40,\n '[': 41,\n 'L': 42,\n '7': 43,\n '2': 44,\n 'e': 45,\n 'q': 46,\n 'n': 47,\n '|': 48,\n 'i': 49,\n 't': 50,\n 'D': 51,\n '`': 52,\n '_': 53,\n 'x': 54,\n 'C': 55,\n '-': 56,\n 'y': 57,\n '4': 58,\n 'V': 59,\n '3': 60,\n 'c': 61,\n 'J': 62,\n 'r': 63,\n 'S': 64,\n 'E': 65,\n 'd': 66,\n 'M': 67,\n 'N': 68,\n 'T': 69,\n 'v': 70,\n 'w': 71,\n ';': 72,\n 'Q': 73,\n '5': 74,\n '1': 75,\n ')': 76,\n '0': 77,\n 's': 78,\n '}': 79,\n 'O': 80,\n '9': 81,\n 'B': 82,\n '?': 83}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# letter -> number (Encoder takes a letter and return the corresponding number for it)\n",
    "encoder = {char: idx for idx, char in decoder.items()} # Dictionary generator.\n",
    "encoder\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "array([36, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n       24, 24, 24, 24, 24, 75, 36, 24, 24,  0, 63, 33,  4, 24, 15, 17, 49,\n       63, 45, 78, 50, 24, 61, 63, 45, 17, 50,  3, 63, 45, 78, 24, 71, 45,\n       24, 66, 45, 78, 49, 63, 45, 24, 49, 47, 61, 63, 45, 17, 78, 45, 34,\n       36, 24, 24, 69,  2, 17, 50, 24, 50,  2, 45, 63, 45, 37, 57, 24, 37,\n       45, 17,  3, 50, 57, 25, 78, 24, 63, 33, 78, 45, 24,  4, 49,  9,  2,\n       50, 24, 47, 45, 70, 45, 63, 24, 66, 49, 45, 34, 36, 24, 24, 82,  3,\n       50, 24, 17, 78, 24, 50,  2, 45, 24, 63, 49, 32, 45, 63, 24, 78,  2,\n       33,  3, 28, 66, 24, 37, 57, 24, 50, 49,  4, 45, 24, 66, 45, 61, 45,\n       17, 78, 45, 34, 36, 24, 24, 40, 49, 78, 24, 50, 45, 47, 66, 45, 63,\n       24,  2, 45, 49, 63, 24,  4, 49,  9,  2, 50, 24, 37, 45, 17, 63, 24,\n        2, 49, 78, 24,  4, 45,  4, 33, 63, 57,  7, 36, 24, 24, 82,  3, 50,\n       24, 50,  2, 33,  3, 24, 61, 33, 47, 50, 63, 17, 61, 50, 45, 66, 24,\n       50, 33, 24, 50,  2, 49, 47, 45, 24, 33, 71, 47, 24, 37, 63, 49,  9,\n        2, 50, 24, 45, 57, 45, 78, 34, 36, 24, 24,  0, 45, 45, 66, 25, 78,\n       50, 24, 50,  2, 57, 24, 28, 49,  9,  2, 50, 25, 78, 24, 15, 28, 17,\n        4, 45, 24, 71, 49, 50,  2, 24, 78, 45, 28, 15, 56, 78,  3, 37, 78,\n       50, 17, 47, 50, 49, 17, 28, 24, 15,  3, 45, 28, 34, 36, 24, 24, 67,\n       17,  6, 49, 47,  9, 24, 17, 24, 15, 17,  4, 49, 47, 45, 24, 71,  2,\n       45, 63, 45, 24, 17, 37,  3, 47, 66, 17, 47, 61, 45, 24, 28, 49, 45,\n       78, 34, 36, 24, 24, 69,  2, 57, 24, 78, 45, 28, 15, 24, 50,  2, 57,\n       24, 15, 33, 45, 34, 24, 50, 33, 24, 50,  2, 57, 24, 78, 71, 45, 45,\n       50, 24, 78, 45, 28, 15, 24, 50, 33, 33, 24, 61, 63,  3, 45, 28,  7,\n       36, 24, 24, 69,  2, 33,  3, 24, 50,  2, 17, 50, 24, 17, 63, 50, 24,\n       47, 33, 71, 24, 50,  2, 45, 24, 71, 33, 63, 28, 66, 25, 78, 24, 15,\n       63, 45, 78,  2, 24, 33, 63, 47, 17,  4, 45, 47, 50, 34, 36, 24, 24,\n       19, 47, 66, 24, 33, 47, 28, 57, 24,  2, 45, 63, 17, 28, 66, 24, 50,\n       33, 24, 50,  2, 45, 24,  9, 17,  3, 66, 57, 24, 78, 32, 63, 49, 47,\n        9, 34, 36, 24, 24, 10, 49, 50,  2, 49, 47, 24, 50,  2, 49, 47, 45,\n       24, 33, 71, 47, 24, 37,  3])"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the entire text with encoder.\n",
    "encoded_text = np.array([encoder[char] for char in text])\n",
    "encoded_text[:500]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "# Declare a function for one-hot encoding.\n",
    "def one_hot_encoder(encoded_text, number_of_unique_chars):\n",
    "\n",
    "    # encoded_text -> batch of encoded text\n",
    "    # number_of_unique_chars -> len(set(text))\n",
    "\n",
    "    one_hot = np.zeros((encoded_text.size, number_of_unique_chars), dtype=np.float32)\n",
    "    # OR use \"one_hot = one_hot.astype(np.float32)\"\n",
    "    # Use the 'dtype=float32' to get precision on PyTorch.\n",
    "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.\n",
    "    # FANCY INDEXING = Passing an array of indices to access multiple array elements at once.\n",
    "    one_hot = one_hot.reshape((*encoded_text.shape, number_of_unique_chars)) # Not mandatory.\n",
    "    return one_hot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0 1 3]\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "ex = np.array([1, 2, 0, 1, 3])\n",
    "print(ex)\n",
    "print(one_hot_encoder(ex, 5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 0., 1.]], dtype=float32)"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh = np.zeros((5, 4), dtype=np.float32)\n",
    "# FANCY INDEXING = Passing an array of indices to access multiple array elements at once.\n",
    "oh[np.arange(oh.shape[0]), ex.flatten()] = 1.\n",
    "oh.reshape(5, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# ###########################################################################################################\n",
      "#  NLP with PyTorch - Generating Training Batches\n",
      "# ###########################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "# ###########################################################################################################\n",
    "#  NLP with PyTorch - Generating Training Batches\n",
    "# ###########################################################################################################\n",
    "''')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text, sample_per_batch=10, sequence_length=50):\n",
    "\n",
    "    # X : 'encoded_text' of length 'sequence_length' -> [0, 1, 2], [1, 2, 3]\n",
    "    # Y : 'encoded_text' shifted by one to right.    -> [1, 2, 3], [2, 3, 4] : shifted to right by one index.\n",
    "\n",
    "    # Number of characters per batch\n",
    "    chars_per_batch = sample_per_batch * sequence_length\n",
    "    # Number of batches available.\n",
    "    number_of_batches = int(len(encoded_text) / chars_per_batch)\n",
    "\n",
    "    # Cut-off the end of the encoded_text that won't fit evenly into a batch (Loss little bit info).\n",
    "    encoded_text = encoded_text[:number_of_batches * chars_per_batch]\n",
    "\n",
    "    encoded_text = encoded_text.reshape(sample_per_batch, -1)\n",
    "\n",
    "    for n in range(0, encoded_text.shape[1], sequence_length):\n",
    "\n",
    "        x = encoded_text[:, n:n + sequence_length]\n",
    "        # Create a 'x' like zeros array.\n",
    "        y = np.zeros_like(x)\n",
    "\n",
    "        try:\n",
    "\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1] = encoded_text[:, n + sequence_length]\n",
    "\n",
    "        except IndexError:\n",
    "\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1] = encoded_text[:, 0]\n",
    "\n",
    "        yield  x, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19])"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sample = np.arange(20)\n",
    "my_sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[ 0,  1,  2,  3,  4],\n        [10, 11, 12, 13, 14]]),\n array([[ 1,  2,  3,  4,  5],\n        [11, 12, 13, 14, 15]]))"
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_batch_generator = generate_batches(my_sample, sample_per_batch=2, sequence_length=5)\n",
    "x, y = next(my_batch_generator)\n",
    "x, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# ###########################################################################################################\n",
      "#  NLP with PyTorch - Creating the LSTM Model\n",
      "# ###########################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "# ###########################################################################################################\n",
    "#  NLP with PyTorch - Creating the LSTM Model\n",
    "# ###########################################################################################################\n",
    "''')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "class CharacterModel(nn.Module):\n",
    "\n",
    "    def __init__(self, all_unique_chars, hidden_size=256, num_layers=4, drop_p=0.5, use_gpu=False):\n",
    "        super().__init__()\n",
    "        self.drop_p = drop_p\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        self.all_chars = all_unique_chars\n",
    "        self.decoder = dict(enumerate(all_unique_chars))\n",
    "        self.encoder = {char: idx for idx, char in self.decoder.items()}\n",
    "\n",
    "        self.lstm = nn.LSTM(len(all_unique_chars),\n",
    "                            hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            dropout=drop_p,\n",
    "                            batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc_linear = nn.Linear(hidden_size, len(all_unique_chars))\n",
    "\n",
    "    def forward(self, X, hidden):\n",
    "\n",
    "        lstm, hidden = self.lstm(X, hidden)\n",
    "        lstm = self.dropout(lstm)\n",
    "        lstm = lstm.contiguous().view(-1, self.hidden_size)\n",
    "        return self.fc_linear(lstm), hidden\n",
    "\n",
    "    def hidden_state(self, batch_size):\n",
    "\n",
    "        if self.use_gpu:\n",
    "            hidden = (torch.zeros(self.num_layers, batch_size, self.hidden_size).cuda(),\n",
    "                      torch.zeros(self.num_layers, batch_size, self.hidden_size).cuda())\n",
    "        else:\n",
    "            hidden = (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "                      torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "        return hidden"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "data": {
      "text/plain": "CharacterModel(\n  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc_linear): Linear(in_features=512, out_features=84, bias=True)\n)"
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CharacterModel(all_unique_chars, hidden_size=512, num_layers=3)\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "data": {
      "text/plain": "5470292"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for p in model.parameters():\n",
    "    total += p.numel()\n",
    "total\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "4901048"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_percent = 0.9\n",
    "train_idx = int(len(encoded_text) * train_percent)\n",
    "train_idx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "training_data = encoded_text[:train_idx]\n",
    "test_data = encoded_text[train_idx:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# ###########################################################################################################\n",
      "#  NLP with PyTorch - Training LSTM Model\n",
      "# ###########################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "# ###########################################################################################################\n",
    "#  NLP with PyTorch - Training LSTM Model\n",
    "# ###########################################################################################################\n",
    "''')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "epochs = 60\n",
    "batch_size = 100\n",
    "seq_length = 100\n",
    "\n",
    "tracker = 0\n",
    "\n",
    "num_chars = max(encoded_text) + 1 # + 1 because indexes start at zero."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "# Set to training mode.\n",
    "model.train()\n",
    "\n",
    "if model.use_gpu:\n",
    "    model = model.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 25 -> validation loss -> 3.201533794403076\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-134-f92eef284269>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     52\u001B[0m                 \u001B[0mval_hidden\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtuple\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mstate\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mval_hidden\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 53\u001B[1;33m                 \u001B[0mlstm\u001B[0m \u001B[1;33m,\u001B[0m \u001B[0mval_hidden\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_hidden\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     54\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m                 \u001B[0mval_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlstm\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch_size\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mseq_length\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlong\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\pytorchdev\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    548\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    549\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 550\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    551\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    552\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-119-91518e4a4d6f>\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, X, hidden)\u001B[0m\n\u001B[0;32m     22\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhidden\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 24\u001B[1;33m         \u001B[0mlstm\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhidden\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlstm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhidden\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     25\u001B[0m         \u001B[0mdropout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdropout\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlstm\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m         \u001B[0mdropout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdropout\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcontiguous\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhidden_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\pytorchdev\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    548\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    549\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 550\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    551\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    552\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\pytorchdev\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input, hx)\u001B[0m\n\u001B[0;32m    567\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcheck_forward_args\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_sizes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    568\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mbatch_sizes\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 569\u001B[1;33m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001B[0m\u001B[0;32m    570\u001B[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001B[0;32m    571\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "\n",
    "    hidden = model.hidden_state(batch_size)\n",
    "\n",
    "    for x, y in generate_batches(encoded_text, batch_size, seq_length):\n",
    "\n",
    "        tracker += 1\n",
    "\n",
    "        x = one_hot_encoder(x, num_chars)\n",
    "\n",
    "        x = torch.tensor(x)\n",
    "        target = torch.tensor(y)\n",
    "\n",
    "        if model.use_gpu:\n",
    "            x = x.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        # Reset the hidden state. to avoid back propagation of hidden layer.\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        lstm, hidden = model(x, hidden)\n",
    "        loss = criterion(lstm, target.view(batch_size * seq_length).long())\n",
    "\n",
    "        # Back-propagate before 'gradient clipping'.\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping to avoid gradient explosions.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if tracker % 25 == 0:\n",
    "\n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "\n",
    "            for x, y in generate_batches(test_data, batch_size, seq_length):\n",
    "\n",
    "                x = one_hot_encoder(x, num_chars)\n",
    "\n",
    "                x = torch.tensor(x)\n",
    "                target = torch.tensor(y)\n",
    "\n",
    "                if model.use_gpu:\n",
    "                    x = x.cuda()\n",
    "                    target = target.cuda()\n",
    "\n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                lstm , val_hidden = model(x, val_hidden)\n",
    "\n",
    "                val_loss = criterion(lstm, target.view(batch_size * seq_length).long())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            print(f'epoch: {i}, step: {tracker} -> validation loss -> {val_loss.item()}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the trained model.\n",
    "model_name = 'shakespeare_model.net___'\n",
    "torch.save(model.state_dict(), model_name)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# ###########################################################################################################\n",
      "#  NLP with PyTorch - Generating Predictions\n",
      "# ###########################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "# ###########################################################################################################\n",
    "#  NLP with PyTorch - Generating Predictions\n",
    "# ###########################################################################################################\n",
    "''')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change 'map_location' to CPU hence the model trained with GPU.\n",
    "model = model.load_state_dict(torch.load(f='shakespeare_model.net', map_location=torch.device('cpu')))\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [],
   "source": [
    "def predict_next_char(model, char, hidden=None, k=1):\n",
    "\n",
    "    encoded_text = model.encoder[char]\n",
    "\n",
    "    encoded_text = np.array([[encoded_text]])\n",
    "\n",
    "    encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
    "\n",
    "    inputs = torch.tensor(encoded_text)\n",
    "\n",
    "    # if model.use_gpu:\n",
    "    #     inputs = inputs.cuda()\n",
    "\n",
    "    hidden = tuple([state.data for state in model.hidden])\n",
    "\n",
    "    lstm_out, hidden = model(inputs, hidden)\n",
    "\n",
    "    probs = F.softmax(lstm_out, dim=1)\n",
    "\n",
    "    # If the network is using a GPU for training and validation, probabilities returns by the model\n",
    "    # should move back to the CPU, in order to further use with numpy.\n",
    "    if model .use_gpu:\n",
    "        probs = probs.cpu()\n",
    "\n",
    "    # 'topk(int)' we can use to define how many top matching outputs we need consider from the probability\n",
    "    # tensor\n",
    "    probs, index_positions = probs.topk(k)\n",
    "\n",
    "    index_positions = index_positions.numpy().squeeze()\n",
    "    probs = probs.numpy().flatten()\n",
    "    # Probabilities per index.\n",
    "    probs = probs / probs.sum()\n",
    "\n",
    "    char = np.random.choice(index_positions, p=probs)\n",
    "\n",
    "    return model.decoder[char], hidden"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [],
   "source": [
    "def generate_text(model, future_predicts, seed='The', k=1):\n",
    "\n",
    "    # if model.use_gpu:\n",
    "    #     model = model.cuda()\n",
    "    # else:\n",
    "    #     model = model.cpu()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    output_chars = [c for c in seed]\n",
    "    hidden = model.hidden_state(batch_size=1)\n",
    "\n",
    "    for char in seed:\n",
    "        char, hidden = predict_next_char(model, char, hidden, k=k)\n",
    "\n",
    "    output_chars.append(char)\n",
    "\n",
    "    for i in range(future_predicts):\n",
    "        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
    "\n",
    "        output_chars.append(char)\n",
    "\n",
    "    return ''.join(output_chars)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_IncompatibleKeys' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-162-3bae4013fc9b>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgenerate_text\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1000\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mseed\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'The'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mk\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-161-c30696244e26>\u001B[0m in \u001B[0;36mgenerate_text\u001B[1;34m(model, future_predicts, seed, k)\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[1;31m#     model = model.cpu()\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m     \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meval\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[0moutput_chars\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mc\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mc\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mseed\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: '_IncompatibleKeys' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, 1000, seed='The', k=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-a503b19",
   "language": "python",
   "display_name": "PyCharm (pytorch-tute)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}